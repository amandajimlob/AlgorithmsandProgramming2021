---
title: "Algorithms and programming 2021"
author: Amana Jiménez Lobato
output:
  html_document:
    df_print: paged
  pdf_document: default
---

# 78.5% (39.25/50)

## EXPLANATION OF THE TEST:  

Welcome to the final exam of Algorithms and Programming course for 2021. Good job for making it this far! We hope you had fun and learned a lot during this course.  
  
The exam is a bit different from the rest of the homeworks. Four exercises are offered, but you only need to solve 3 of them.  
  
  * Exercise 1 is a simulation of a sequencing experiment, and downstream analysis of the data.
  * Exercise 2 focuses on implementing an algorith in R by writing your own function.
  * Exercise 3 is special. You have 2 options from which you can choose: there is a very easy version which can earn you 10 points, and there is a difficult version for 17.5 points. You need to solve only one of them. Of course, by choosing the easy version you can get maximum 42.5 out of 50 points (85%), but it's more likely you will get them since the exercise is the easiest in the exam. The difficult version is the most difficult exercise in this exam but by solving it you can get the maximum number of points. If you aim to have an A in this course, you will probably need to solve the difficult version. If you don't aim for an A, you can choose the easier version.

## RULES:  

1. Solve exercises 1 (17.5 pts), 2 (15 pts) and ONLY ONE VERSION of exercise 3 (10 pts or 17.5 pts).  

2. Send all questions you have regarding this exam directly by email to dglavas@bioinfo.hr or pstancl@bioinfo.hr. Also, sending direct messages to us on Bepo is allowed. We will answer to each one of you directly and post the questions and answers to the forum if appropriate.

3. Don't work in groups and exchange ideas. Ask the assistants if something is unclear, not each other. If we find suspiciously similar solutions to some exercises, we will divide the total points you would have gotten for the exercise by the number of people that have (or have ever written) this code.

4. Don't copy paste the solutions from the internet. Don't download any additional data from the internet. If you think something is missing, ask the assistants directly.  

5.  Allowed packages are (Ask if you need to load any other package that is not on this list.) :  
data.table
stringr  
ggplot2  
Biostrings  
GenomicRanges  
dplyr  
BSGenome (+mouse and human if needed)  
biomaRt  
seqLogo  
```{r, echo = FALSE, message=FALSE, warning=FALSE}
library(data.table)
library(stringr)
library(ggplot2)
library(Biostrings)
library(GenomicRanges)
library(dplyr)
library(BSgenome)
library(biomaRt)
library(seqLogo)
```

6. You can use any of the allowed packages as you wish. You can also use base R as you wish, unless specified otherwise. You are allowed to use traditional loops as well as apply family of functions, but try to keep the loops at minimum. Now when you've reached a certain level in your programming skills, we expect your code to be tidy, relatively elegant, and run in an reasonable amount of time (in addition to producing a correct result, of course). So use loops as a last resort, only if you've already tried to solve something using apply and failed.   

7. You have learning various functions that solve a particular question of interest. First you must use the learned function or existing function from the learnt packages before you go writing your own function.



## Exercise 1: DNA sequencing simulation with errors  
#### 17.5 points  
  
Let's say that you have some piece of DNA that you want to sequence.  
  
#### Part 1: Getting the reads  
  
Your goal is to simulate the final reads you get in the experiment when you sequence the starting DNA with 20x coverage. The sequencing technology that you are using creates reads of 100bp +/- 6bp length (avg +/- sd, normally distributed). Any given base that you read has 0.3% chance of being read incorrectly. Reads are being made starting from random positions within the genome. Base coverage is defined as how many times a given position (base) in your starting sequence is present in the reads, on average.  
  
#### Part 2: Inspecting the kmers  
  
After you have the reads, create kmers of length kLen from the reads. Kmer, in this context, is just a window (of length kLen) into read sequence. For example (we will presume that the read has 15 bases for the sake of brevity, but this is highly unlikely to occur with 100 +/- 6 bp distribution):  
  
read:  
  
ATGCCGTAAATTCCG  
  
kmers (kLen=12):  
  
ATGCCGTAAATT   
  TGCCGTAAATTC   
    GCCGTAAATTCC   
      CCGTAAATTCCG   
  
After you have kmers for all the reads, plot the kmer coverage in the following way: X-axis should represent how many times a kmer is present in the list of all kmers; Y-axis should represent how many such kmers there are.  
  
Create a function my.kmer.coverage that does this simulation (part 1 + part 2, also simulate errors in reads) and plotting. It should have the following arguments: sequence (one DNAString), kLen (the length of kmers) and coverage (base coverage, explained above).  
Try it out on a part of Methicillin-resistant Staphylococcus aureus genome, strain MRSA252. You can find the FASTA file of the whole genome here: ftp://ftp.sanger.ac.uk/pub/project/pathogens/sa/MRSA252.dna (don't try to open it in browser, load it directly into R using the link). Try to make coverage around 20 and kLen around 20, and set the sequence length you use in a way that running the whole exercise takes 2 minutes or less.  
```{r}
S.aureus_genome <- readDNAStringSet("ftp://ftp.sanger.ac.uk/pub/project/pathogens/sa/MRSA252.dna")
S.aureus_genome
```
```{r}
my.kmer.coverage <- function(seq, kLen, cov){    ### not a good idea to call a variable "seq" because that's already a function name - it will rarely cause an actual problem but you will still have to be extra-careful because of it, and it will confuse other readers of your code
  #set.seed(100)
  
  # To know how many reads we need: coverage = (read count * read length ) / total genome size --> read count = (coverage * total genome size)/read length
  n <- round(cov*nchar(seq)/100)
  lengths <- round(rnorm(n, 100, 6))
  starting_pos <- round(runif(n, 1, nchar(seq)-100))
  
  # Function to make that any given base that you read has 0.3% chance of being read incorrectly:
  misreads <- function(reads){
    occurences <- rbinom(nchar(reads),size = 1, prob = 0.003)
    occurences_pos <- ifelse(length(unique(occurences))>1, which(occurences > 0), 0) #If length(unique()) is not = 0, this would mean that there are more values than 0
    if(occurences_pos!=0){ 
      replace_with <- sample(c("A", "T", "G", "C"), length(occurences_pos), replace = TRUE)   ### see comment below!
      str_read <- DNAString(reads)
      return(as.character(replaceLetterAt(str_read, occurences_pos, replace_with)))
    }
    else{
      return(as.character(reads))
    }
  }
  obtain_kmers <- function(reads, kLen){
    str_read <- DNAString(reads)
    Kmers <- list()
    Kmers[[1]] <- DNAString(str_sub(reads, 1, 1+kLen-1))
    i = 2
    while(i<=nchar(reads)-kLen+1){   ### this will be a bit slow unnecessarily - some kind of apply would be better than while, and DNAString subsetting with multiple starts and ends at the same time would be even better than that. or use Views().
      Kmers[[i]] <- DNAString(str_sub(reads, i, i+kLen-1))
      i <- i+1
    }
    DNAStringSet(unlist(Kmers))
  }
  
reads <- DNAStringSet(str_sub(seq, starting_pos, starting_pos+lengths-1))
reads_dt <- data.table(width = width(reads), reads_no_misreads = as.character(reads)) %>% rowwise() %>% summarise(width, reads_no_misreads, reads_with_misreads = misreads(reads = reads_no_misreads)) 

Kmers <- unlist(lapply(reads_dt$reads_with_misreads, obtain_kmers, kLen = 20))
Kmers_dt <- data.table(width = kLen, Kmers = unlist(lapply(1:length(Kmers), function(x){as.character(Kmers[[x]])}))) %>% mutate(reads = unlist(lapply(1:nrow(reads_dt),function(x){rep(reads_dt$reads_with_misreads[x], (reads_dt$width[x] - kLen + 1))})))
### The line above is a bit messy - typical example of salami code and very hard to read and untangle. Try to avoid this kind of code because when some time passes, you'll have as much problem as anybody else deciphering what's going on here and why. There are better ways for saving everything into a data.table. Also the reads column which you create in the last step is unnecessary (you don't use it later) and it takes time to run. 

Kmers_dt[order(Kmers), kmer_id:=rleid(Kmers)]
Kmers_dt_for_plot <- Kmers_dt[,.(Kmers,.N), by=kmer_id] [order(kmer_id), .(unique(Kmers)), by=.(kmer_id, N)]

ggplot(Kmers_dt_for_plot, aes(N)) + geom_bar() + theme_light() + xlab("nº of times kmer is present") + coord_cartesian(xlim=c(0,50)) 

}
prueba<-S.aureus_genome[[1]][1:5000]
my.kmer.coverage(prueba, 20, 20)
```

*The way you're introducing sequencing errors won't result in 0.3% error rate. You pick 0.3% of positions, but since you're choosing the substitute base randomly from all 4 bases, you'll get the same base as substitution in approx. 1/4 of the cases. I saw later that you're aware of this and corrected it in another part of the task so you didn't lose as many points as you normally would on this.*
*Also runs pretty slow due to while(). 10.25/11.5 for Pt1&2*

#### Part 3: Conclusions    
  
Can you distinguish between kmers that have an error and error-free kmers? If you had a diploid organism, could you distinguish between kmers with SNPs (bases that differ in homologuous chromosomes) and errors? Explain how you would do this and show on an example (get 2 DNA strings as input instead of one, and make them differ in 5% of sites to represent SNPs). 

- Can you distinguish between kmers that have an error and error-free kmers?

I find two ways of interpretating this exercise: extracting the kmers I already created that have an error as a result of the random misreading of reads (with probability of 0.3%), or finding a qualitative way of distinguishing the kmers that have mistakes.

I. First interpretation:
```{r}
# I copy the same function, but with the probability of a misread as a parameter, so I can make this probability bigger, and in this case I want it to return a data.table with the reads and their kmers:
my.kmer.coverage <- function(seq, kLen, cov, error_prob){
  #set.seed(100)
  
  n <- round(cov*nchar(seq)/100)
  lengths <- round(rnorm(n, 100, 6))
  starting_pos <- round(runif(n, 1, nchar(seq)-100))
  
  # Function to make that any given base that you read has 0.3% chance of being read incorrectly:
  misreads <- function(reads){
    occurences <- rbinom(nchar(reads),size = 1, prob = error_prob)
    occurences_pos <- ifelse(length(unique(occurences))>1, which(occurences > 0), 0) #If length(unique()) is not = 0, this would mean that there are more values than 0
    if(occurences_pos!=0){ 
      replace_with <- sample(c("A", "T", "G", "C"), length(occurences_pos), replace = TRUE)
      str_read <- DNAString(reads)
      return(list(occurences_pos, as.character(replaceLetterAt(str_read, occurences_pos, replace_with))))
    }
    else{
      return(list(occurences_pos, as.character(reads)))
    }
  }
  obtain_kmers <- function(reads, kLen){
    str_read <- DNAString(reads)
    Kmers <- list()
    Kmers[[1]] <- DNAString(str_sub(reads, 1, 1+kLen-1))
    i = 2
    while(i<=nchar(reads)-kLen+1){
      Kmers[[i]] <- DNAString(str_sub(reads, i, i+kLen-1))
      i <- i+1
    }
    DNAStringSet(unlist(Kmers))
  }
  
  reads <- DNAStringSet(str_sub(seq, starting_pos, starting_pos+lengths-1))

  reads_dt <- data.table(width = width(reads), reads_no_misreads = as.character(reads)) %>% rowwise() %>% summarise(width, reads_no_misreads, reads_with_misreads = (misreads(reads_no_misreads))[[2]], misreads_pos = (misreads(reads_no_misreads))[[1]]) 

  Kmers <- unlist(lapply(reads_dt$reads_with_misreads, obtain_kmers, kLen = 20))
  Kmers_dt <- data.table(width = kLen, Kmers = unlist(lapply(1:length(Kmers), function(x){as.character(Kmers[[x]])}))) %>% mutate(reads = unlist(lapply(1:nrow(reads_dt),function(x){rep(reads_dt$reads_with_misreads[x], (reads_dt$width[x] - kLen + 1))})))

  inner_join(reads_dt, Kmers_dt, by = c("reads_with_misreads" = "reads"))
}

test <- S.aureus_genome[[1]][1:5000]
dt <- my.kmer.coverage(test, 20, 20, 0.003)

```  
```{r}
# With the table I extracted, I want to see the kmers from each read that actually include the position where the misread happened while making the reads:
width_reads <- dt %>% filter(misreads_pos!=0) %>%   group_by(reads_with_misreads, width.x) %>% arrange(width.x) %>% filter(row_number()==1) %>% dplyr::select(width.x, width.y) 

kmers_starts <- unlist(lapply(width_reads$width.x, function(x, y){ seq(1, x - y + 1, by =1)}, y = unique(width_reads$width.y)))
kmers_ends <- unlist(lapply(kmers_starts, function(x, y){ x + y - 1 }, y = unique(width_reads$width.y)))
kmers_with_misreads <- dt %>% filter(misreads_pos!=0) %>% arrange(width.x) %>% mutate(kmers_starts = kmers_starts, kmers_ends = kmers_ends) %>% filter(kmers_starts < misreads_pos & kmers_ends > misreads_pos)
kmers_with_misreads
```
*This approach wouldn't make sense because with real-world data you wouldn't know the misreads' positions (at least not initially, before mapping). Furthermore, the reads are a starting point for almost any analysis, not the kmers - so extracting erroneous kmers doesn't have an effect on pretty much anything you do downstream. On the other hand, if you were to discard whole reads, you lose all the "good" information in them. In real-life applications, this type of analysis would be used more as a diagnostic tool and not a filtering one.*
*And keep in mind that you don't always have a reference sequence when sequencing something! (or know all variation)*

II. Second interpretation:
```{r}
# By increasing the number of misreads and comparing the plots, we should be able to see where the misreads fall in the kmers:
my.kmer.coverage <- function(seq, kLen, cov, error_prob){
  #set.seed(100)
  
  n <- round(cov*nchar(seq)/100)
  lengths <- round(rnorm(n, 100, 6))
  starting_pos <- round(runif(n, 1, nchar(seq)-100))
  
  # Function to make that any given base that you read has 0.3% chance of being read incorrectly:
  misreads <- function(reads){
    occurences <- rbinom(nchar(reads),size = 1, prob = error_prob)
    occurences_pos <- ifelse(length(unique(occurences))>1, which(occurences > 0), 0) #If length(unique()) is not = 0, this would mean that there are more values than 0
    if(occurences_pos!=0){ 
      replace_with <- sample(c("A", "T", "G", "C"), length(occurences_pos), replace = TRUE)
      str_read <- DNAString(reads)
      return(as.character(replaceLetterAt(str_read, occurences_pos, replace_with)))
    }
    else{
      return(as.character(reads))
    }
  }
  obtain_kmers <- function(reads, kLen){
    str_read <- DNAString(reads)
    Kmers <- list()
    Kmers[[1]] <- DNAString(str_sub(reads, 1, 1+kLen-1))
    i = 2
    while(i<=nchar(reads)-kLen+1){
      Kmers[[i]] <- DNAString(str_sub(reads, i, i+kLen-1))
      i <- i+1
    }
    DNAStringSet(unlist(Kmers))
  }
  
  reads <- DNAStringSet(str_sub(seq, starting_pos, starting_pos+lengths-1))
  reads_dt <- data.table(width = width(reads), reads_no_misreads = as.character(reads)) %>% rowwise() %>% summarise(width, reads_no_misreads, reads_with_misreads = misreads(reads = reads_no_misreads)) 

  Kmers <- unlist(lapply(reads_dt$reads_with_misreads, obtain_kmers, kLen = 20))
  Kmers_dt <- data.table(width = kLen, Kmers = unlist(lapply(1:length(Kmers), function(x){as.character(Kmers[[x]])}))) %>% mutate(reads = unlist(lapply(1:nrow(reads_dt),function(x){rep(reads_dt$reads_with_misreads[x], (reads_dt$width[x] - kLen + 1))})))

  Kmers_dt[order(Kmers), kmer_id:=rleid(Kmers)]
  Kmers_dt_for_plot <- Kmers_dt[,.(Kmers,.N), by=kmer_id] [order(kmer_id), .(unique(Kmers)), by=.(kmer_id, N)]

  ggplot(Kmers_dt_for_plot, aes(N)) + geom_bar() + theme_light() + xlab("nº of times kmer is present") + coord_cartesian(xlim=c(0,50)) 

}
test<-S.aureus_genome[[1]][1:25000]
my.kmer.coverage(test, 20, 20, 0.1)

```
When we increase the probability of a mistake reading a base of the sequence when obtaining the readings, we can see how the peak in 1 gets higher: the chances of getting the same kmer more than once decreases, since the mistakes make the reds more and more different, and subsequently, also the kmers. So we can suspect that the kmers that are found only once might have some misread, since by the way we obtained the reads and the kmers, there is quite a possibility that some kmers are the same. There is no doubt that the kmers that appeared more than once before, and now with a higher probability of mistakes, are found just once, are kmers with mosreads.
*nice explanation*

- If you had a diploid organism, could you distinguish between kmers with SNPs (bases that differ in homologuous chromosomes) and errors? Explain how you would do this and show on an example (get 2 DNA strings as input instead of one, and make them differ in 5% of sites to represent SNPs).
```{r}
# If the funtion already changes the sequence inside to introduce the SNPs, then I just need 1 sequence as input in the function:
# I am also goin to change my function a little bit, so the sequences differ exactly in 5% of the sites (with the code I did before, the pobability coul be a little bit lower)

my.kmer.coverage <- function(seq, kLen, cov, differ_perc){
  
  make_differences <- function(seq, differ_perc){
    seq <- DNAString(seq)
    occurences <- rbinom(nchar(seq),size = 1, prob = differ_perc)
    occurences_pos <- ifelse(length(unique(occurences))>1, which(occurences > 0), 0) #If length(unique()) is not = 0, this would mean that there are more values than 0   ### ok although sum(occurences)>0 would be faster than length(unique())
    original_letter <- c()
    if(occurences_pos!=0){ 
      for(i in 1:length(occurences_pos)){
        letter<-as.character(subseq(seq,occurences_pos[i], occurences_pos[i]))
        original_letter <- c(original_letter,letter)
      }
      original_letter
    }
    
    #Nested function to generate random letters to replace the originals (now I tried to avoid that one letter it's substituted by itself when it should introduce a mismatch):
    replacement_letters<-function(letter){
      replacement <- c()
      for (i in 1:length(letter)){    ### this is ok (although a bit slow) and it ensures that an error/SNP is introduced in specified % of cases - why didn't you include this in Pt1&2?
        if (letter[i]=="A"){
          new_letter <- sample(c("T","G","C"),1,1)
          replacement <- c(replacement,new_letter)
        }
        if(letter[i]=="T"){
          new_letter <- sample(c("A","G","C"),1,1)
          replacement <- c(replacement,new_letter)
        }
        if(letter[i]=="G"){
          new_letter <- sample(c("A","T","C"),1,1)
          replacement <- c(replacement,new_letter)
        }
        if(letter[i]=="C"){
          new_letter <- sample(c("A","T","G"),1,1)
          replacement <- c(replacement,new_letter)
        }
      }
      all_replacements <- paste(replacement, collapse="")  
    }
    if(length(original_letter)!=0){
      all_replacements <- replacement_letters(original_letter)
      as.character(replaceLetterAt(seq, occurences_pos, all_replacements))
    }
    else{
      as.character(seq)
    }
  }  

  seq_with_SNPs <- make_differences(seq, 0.05) #str_count(compareStrings(seq, seq_with_SNPs), pattern = "\\?")/nchar(seq)*100 to make sure the difference percentage is around 5%
    
  n <- round(cov*nchar(seq)/100)
  
  lengths1 <- round(rnorm(n, 100, 6))
  starting_pos1 <- round(runif(n, 1, nchar(seq)-100))
  reads_original <- DNAStringSet(str_sub(seq, starting_pos1, starting_pos1+lengths1-1))
  reads_original_dt <- data.table(width = width(reads_original), reads_no_misreads = as.character(reads_original)) %>% rowwise() %>% summarise(width, reads_no_misreads, reads_with_misreads = make_differences(reads_no_misreads, 0.003)) %>% mutate(read_type = "with errors")
  
  lengths2 <- round(rnorm(n, 100, 6))
  starting_pos2 <- round(runif(n, 1, nchar(seq)-100))
  reads_with_SNPs <- DNAStringSet(str_sub(seq_with_SNPs, starting_pos2, starting_pos2+lengths2-1))
  reads_with_SNPs_dt <- data.table(width = width(reads_with_SNPs), reads_no_misreads = as.character(reads_with_SNPs)) %>% mutate(reads_with_misreads = NA, read_type = "with SNPs") #to see the differences between misreads and SNPs, we don't introduce misreads in the reads of the string with SNPs.   ### You should've introduced them here as well - we're trying to simulate sequencing data, and there will always be sequencing errors. The point is to distinguish errors from SNPs.

  reads_dt <- merge(reads_original_dt, reads_with_SNPs_dt, all = TRUE)
  
  obtain_kmers <- function(reads, kLen){
    str_read <- DNAString(reads)
    Kmers <- list()
    Kmers[[1]] <- DNAString(str_sub(reads, 1, 1+kLen-1))
    i = 2
    while(i<=nchar(reads)-kLen+1){
      Kmers[[i]] <- DNAString(str_sub(reads, i, i+kLen-1))
      i <- i+1
    }
    DNAStringSet(unlist(Kmers))
  }

  Kmers_original <- unlist(lapply(reads_original_dt$reads_with_misreads, obtain_kmers, kLen = 20))
  Kmers_original_dt <- data.table(width = kLen, Kmers = unlist(lapply(1:length(Kmers_original), function(x){as.character(Kmers_original[[x]])}))) %>% mutate(reads = unlist(lapply(1:nrow(reads_original_dt),function(x){rep(reads_original_dt$reads_with_misreads[x], (reads_original_dt$width[x] - kLen + 1))}))) %>% mutate(read_type = "errors")
  
  Kmers_SNPs <- unlist(lapply(reads_with_SNPs_dt$reads_no_misreads, obtain_kmers, kLen = 20))
  Kmers_SNPs_dt <- data.table(width = kLen, Kmers = unlist(lapply(1:length(Kmers_SNPs), function(x){as.character(Kmers_SNPs[[x]])}))) %>% mutate(reads = unlist(lapply(1:nrow(reads_with_SNPs_dt),function(x){rep(reads_with_SNPs_dt$reads_no_misreads[x], (reads_with_SNPs_dt$width[x] - kLen + 1))}))) %>% mutate(read_type = "SNPs")
  
  Kmers_dt <- merge(Kmers_original_dt, Kmers_SNPs_dt, all = TRUE)
  
  Kmers_dt[order(Kmers), kmer_id:=rleid(Kmers)]
  Kmers_dt_for_plot <- Kmers_dt[,.(Kmers, read_type, .N), by= kmer_id][order(kmer_id), .(unique(Kmers)), by=.(kmer_id, N, read_type)]
  

  ggplot(Kmers_dt_for_plot, aes(N, group = read_type)) + geom_density(aes(fill = read_type), alpha = 0.3) + theme_light() + xlab("nº of times kmer is present") + coord_cartesian(xlim=c(0,50)) 
  
}
test<-S.aureus_genome[[1]][1:10000]
my.kmer.coverage(test, 20, 20, 0.05)
```
The difference between errors in the reads and SNPs in the DNA sequence is that errors happen randomly when doing the reads, so they increase the probability of finding unique kmers, from reads with mistakes. The SNPs are in the original string, so all the reads from that part of the sequence will have that same SNP, that will differ from the reads of the other strand but not between reads of the same strand. 
*it's not strand, it's copy or chromosome!!! "diploid" doesn't mean two-stranded, it means having two copies of genome. plese don't confuse the two.* 
So in the graph we can see how the peak from unique kmers vanishes, and we have a bigger proportion of kmers that are present a couple of times in the case of reads of the strand with SNPs (but without mistakes), and in the case of the strand without SNPs, but reads with errors, this peak is quite high, since all the random errors will cause an increase in the number of unique reads and kmers from those reads.
*Ok, you explained part of it well but kind of missed the point with the plot. How would you distinguish a sequence with errors and the one with SNPs (which would also have errors!) if you sequence them together? You split the plot by data but to point was to see whether we can distinguish erroneous and SNP-containing kmers when they're part of the SAME DATASET - meaning you should've plotted all kmers together in the same way as in first two plots, and then see if you can recognize which ones have errors.*
*One other thing that was worth noting here is the appearance of 2 peaks on the plot. Why do you think that happened?*

*And just a comment on this last chunk: when you're writing a more complex function, the code is often easier to read if you don't "nest" longer functions, but define them outside instead. (this is just practical & stylistic advice though, it has nothing to do with grading)*

*2/3 for Pt3*

#### Part 4: Motif   
  
Install the seqLogo package with BiocManager::install("seqLogo"). Find all the reads which contain the sequence "ATCGTCGAGTC" allowing for maximum 5 mismatches, no indels. Make the position weight matrix (Biostrings package function) and plot the logo for all those matches (seqLogo package). Make the logo for all those matches which have 5 mismatches, 4 mismatches, 3 mismatches, 2 mismatches or 1 mismatch.  
```{r}
obtain_reads <- function(seq, cov, length_reads, error_prob){
  n <- round(cov*nchar(seq)/length_reads)
  lengths <- round(rnorm(n, length_reads, 6))
  starting_pos <- round(runif(n, 1, nchar(seq)-length_reads))
  
  make_differences <- function(seq, error_prob){
    seq <- DNAString(seq)
    occurences <- rbinom(nchar(seq),size = 1, prob = error_prob)
    occurences_pos <- ifelse(length(unique(occurences))>1, which(occurences > 0), 0) #If length(unique()) is not = 0, this would mean that there are more values than 0
    original_letter <- c()
    if(occurences_pos!=0){ 
      for(i in 1:length(occurences_pos)){
        letter<-as.character(subseq(seq,occurences_pos[i], occurences_pos[i]))
        original_letter <- c(original_letter,letter)
      }
      original_letter
    }
    
    #Nested function to generate random letters to replace the originals (now I tried to avoid that one letter it's substituted by itself when it should introduce a mismatch):
    replacement_letters<-function(letter){
      replacement <- c()
      for (i in 1:length(letter)){
        if (letter[i]=="A"){
          new_letter <- sample(c("T","G","C"),1,1)
          replacement <- c(replacement,new_letter)
        }
        if(letter[i]=="T"){
          new_letter <- sample(c("A","G","C"),1,1)
          replacement <- c(replacement,new_letter)
        }
        if(letter[i]=="G"){
          new_letter <- sample(c("A","T","C"),1,1)
          replacement <- c(replacement,new_letter)
        }
        if(letter[i]=="C"){
          new_letter <- sample(c("A","T","G"),1,1)
          replacement <- c(replacement,new_letter)
        }
      }
      all_replacements <- paste(replacement, collapse="")  
    }
    if(length(original_letter)!=0){
      all_replacements <- replacement_letters(original_letter)
      as.character(replaceLetterAt(seq, occurences_pos, all_replacements))
    }
    else{
      as.character(seq)
    }
  }
  
  reads <- DNAStringSet(str_sub(seq, starting_pos, starting_pos+lengths-1))
  reads_dt <- data.table(width = width(reads), reads_no_misreads = as.character(reads)) %>% rowwise() %>% summarise(width, reads_no_misreads, reads_with_misreads = make_differences(reads_no_misreads, error_prob))
  reads_dt
}
reads_test_dt <- as.data.table(obtain_reads(prueba, 20, 100, 0.003)) %>% select_("reads_with_misreads")
reads_test_StringSet <- DNAStringSet(as.character(reads_test_dt$reads_with_misreads))

Logo_plots <- function(dt,n_mismatch){
  dt[,group:=rleid(reads_with_misreads)] 
  pattern <- as.data.table(vmatchPattern("ATCGTCGAGTC", reads_test_StringSet, max.mismatch = n_mismatch)) %>% select_("group", "start", "end", "width")
  
  reads_pattern_merged_dt <- merge(dt, pattern)
  matches <- DNAStringSet(unlist(lapply(1: nrow(reads_pattern_merged_dt), function(x){substr(reads_pattern_merged_dt[x,2], reads_pattern_merged_dt[x,3], reads_pattern_merged_dt[x,4])})))
  bases_proportion <- consensusMatrix(matches,as.prob = T)[1:4,]
  pwm <- makePWM(bases_proportion)
  seqLogo(pwm) 
}
Logo_plots(reads_test_dt,5)
Logo_plots(reads_test_dt,4)
Logo_plots(reads_test_dt,3)
Logo_plots(reads_test_dt,2) # If we allow only 2 or 1 mismatches, there are no coincidences with the given pattern, so we don't obtain any Logo
Logo_plots(reads_test_dt,1)
```
*Nice! I have nothing to add here :)  3/3 for Pt4*  

  
## Exercise 2: k-means clustering
#### 15 points

  
#### Implementing the algorithm in R
  
Create a function that does heuristic k-means clustering (name it My.k.means) and plots the final result (final Voronoi regions should be visible). It should also return positions of the centroids and indication which input point belongs to which centroid. You need to do multiple runs in order to find the consensus centroid position. If points are defined in a space with more than 2 dimensions you do not need to do the plotting. Of course, you are not allowed to use any R function which deals with k-means clustering, from any package. The created function must work on any table or matrix. 

```{r}
#WARNING BEFORE RUNING: Both codes to plot where too extense to include them in this chunk. They are in the next two chunks, right below this one. Run those before.

My.k.means <- function(matrix, k){
  set.seed(100)
  
  # Just a control to check if the clustering is possible with the introduced parameters, and if it is, to convert the inputs to a compatible type for the rest of my code (the function accepts both data.table or data.frames and matrixes or arrays of any dimensions, but I am going to work with data.frames)
  if(dim(matrix)[1]>k){
    if(class(matrix) == "matrix" || class(matrix) == "array"){matrix_df <- data.frame(matrix) }
    else{ matrix_df <- matrix}
  }
  else {stop(); geterrmessage("Number of points to clusters needs to be higher than the number of clusters to achieve")}
  
  n_coordenades <- dim(matrix_df)[2]
  # To keep track of the positios each point had in the original input
  matrix_df <- matrix_df %>% mutate(which_input = seq(1, nrow(.), by = 1))
  
  # I started off generating random clusters with random centroids, just picking k points from the original points input:
  random_inicial_centroids <- sample(1:nrow(matrix_df), k)
  random_inicial_centroids_df <- matrix_df %>% filter(which_input %in% random_inicial_centroids)
  
  # A function I will call repitedly to calculate the euclidean distance between point and centroids of clusters, and then pick the lowerest distance for each point
  function_min_dist <- function(point, centroids_df){
    distances <- c()
    which_i <- c()
    for(i in 1:nrow(centroids_df)){
      euclidean_dist <- sqrt(sum((matrix_df[point, 1:n_coordenades] - centroids_df[i, 1:n_coordenades])^2))
      if(euclidean_dist!=0){ #This is not really necessary since it will only happen in the first run (that the points we took randomly as centroids will match that cluster's centroid and group there, with distance 0 to the centroid) and then it will be corrected in the next runs when recalculating the centroids.
        distances <- c(distances, euclidean_dist)
        which_i <- c(which_i, paste(i))
      }
    }
    which_cluster_min <- which_i[which(distances==min(distances))]
    which_cluster_min
  }
  
  # My function will repeat the process of clustering by the minimal distance from points to centroids until the centroid don't change anymore, which will mean that each point is clustered in its closest cluster with the minimal distance to its centroid. For that, I need to keep a control for the while loop:
  control1 <- list(rep(0, k),rep(0, k))
  control2 <- c(random_inicial_centroids_df[1:k, 1:n_coordenades])
  
  while(isTRUE(all.equal(control1, control2, tolerance = 0)) == FALSE){
    cluster_each_point <- as.numeric(unlist(lapply(1:nrow(matrix_df), function_min_dist, centroids_df = random_inicial_centroids_df)))
    matrix_df_centroids <- matrix_df %>% mutate(cluster_each_point = cluster_each_point) %>% group_by(cluster_each_point) %>% summarise(X_cluster = mean(X1), Y_cluster = mean(X2))
    matrix_df_mod <- matrix_df %>% mutate(n_cluster = cluster_each_point)
    random_inicial_centroids_df <- random_inicial_centroids_df %>% mutate(X1 = replace(X1, seq(1, k, by =1), matrix_df_centroids$X_cluster), X2 = replace(X2, seq(1, k, by =1), matrix_df_centroids$Y_cluster))
    control1 <- control2
    control2 <- c(random_inicial_centroids_df[1:k, 1:n_coordenades])
  }
  
  final_clusters <- random_inicial_centroids_df %>% dplyr::select(X1, X2) %>% mutate(n_cluster = seq(1, k, by=1)) %>% dplyr::rename(x_centroid = X1, y_centroid = X2)
  clustered_points_df <- matrix_df_mod %>% inner_join(final_clusters, by = "n_cluster") %>% dplyr::rename(x_point = X1, y_point = X2)
  
  if(n_coordenades==2){
# I leave the plotting that show the clusters as groups of points as a comment, since I did not really achieve the Voronoi regions:
    
   #ggplot(clustered_points_df, aes(x = x_point, y = y_point, color = as.factor(n_cluster))) + geom_point() + geom_point(aes(x = x_centroid, y = y_centroid, color = "centroid cluster")) + stat_ellipse() + theme_light() + labs(color='Cluster')

#My code for the Voronoi regions is really long, so defined that function in another chunk and just call it here so this function returns the plot as asked.  
    plot_approach1 <- fun_voronoi(clustered_points_df)
    
# So it is my code for the second way of drawing these Voronoi regions, so it is in another chunk as well.
    plot_approach2 <- plot_the_plane_Vdiagram(matrix_df_mod, final_clusters, clustered_points_df)
    
    list(plot_approach1, plot_approach2)
  }
  else { clustered_points_df}
}

# I am going to generate a matrix of 2 dimension points to check the function:
vector1 <- sample (1:1000, 100, replace = TRUE)
vector2 <- sample (1:1000, 100, replace = TRUE)
test_matrix <- array(c(vector1,vector2), dim = c(100,2))
##
test1 <- copy(iris[,1:2])
names(test1) <- NULL
My.k.means(test1 %>% as.matrix(), 3)
##
```
<span style="color:red"> 14/15 Great! Only you had to run the process of generating centroids several times in order to obtain multiple centroids and calculate mean of them to get the consensus. The second approach is absolutely correct. </span>


To know hoy to program the plotting with the Voronoi regions, I first had to make a research of what they are and their mathematical basis. From the information I extracted from the book Plane and Solid Geometry, written by J.M. Aarts (pages 113-125), and from the Wikipedia page https://es.wikipedia.org/wiki/Pol%C3%ADgonos_de_Thiessen (in Spanish), I learned that for obtaining the characteristic polygons of the Voronoi diagrams, I need the tangent lines to the lines that link the centroids (mediatrices, as translated from that page: "They are created by joining the points together by drawing the bisectors of the joining segments. The intersections of these perpendicular bisectors determine a series of polygons in two-dimensional space around a set of control points, such that the perimeter of the polygons generated is equidistant from neighbouring points and designate their area of influence."). So my whole code below has the objective of finding the equations of those tangent lines and plot them.
Ideally, I would also find their intersections, and I would plot the polygons defined by those intersections (with function geom_segment from ggplot and the intersection points), but when I coded this, I obtained a pointless diagram, probably because some error finding the intersections between the lines. So I decided to leave here the code that aproaches the most to what I wanted to obtain, and I will send a picture of what this graph should look like if I had got to stop each line when it cuts another line, so I would have the polugons all these lines define.
```{r}
fun_voronoi <- function(clustered_points){
  centroids_x <- as.numeric(unlist(clustered_points %>% dplyr::select(x_centroid) %>% unique(.)))
  centroids_y <- as.numeric(unlist(clustered_points %>% dplyr::select(y_centroid) %>% unique(.)))

  x_middle <- c()
  y_middle <- c()
  distance <- c()
  x_of_the_centroids <- c()
  y_of_the_centroids <- c()
  for(i in 1:length(centroids_x)){
    for(j in 1:length(centroids_x)){
      if(i!=j){
        x <- (centroids_x[i] + centroids_x[j])/2
        y <- (centroids_y[i] + centroids_y[j])/2
        distance <- c(distance, sqrt((centroids_x[i]-x)^2+(centroids_y[i]-y)^2))
        x_middle <- c(x_middle, x)
        y_middle <- c(y_middle, y)
        x_of_the_centroids <- c(x_of_the_centroids ,centroids_x[i]) 
        y_of_the_centroids <- c(y_of_the_centroids ,centroids_y[i])
      }
    }
  }
  middle_points <- data.frame(x_middle = round(x_middle, digits=3), y_middle = round(y_middle, digits=3), euc_dist = round(distance, digits=3), x_centroid = x_of_the_centroids, y_centroid = y_of_the_centroids)
  middle_points <- middle_points[!duplicated(middle_points[,c('x_middle','y_middle', 'euc_dist')]),]

  mi <- c()
  bi <- c()
  for(i in 1:nrow(middle_points)){
    m <- -(middle_points[i,1] - middle_points[i, 4])/(middle_points[i,2] - middle_points[i, 4])
    mi <- c(mi, -(middle_points[i,1] - middle_points[i, 4])/(middle_points[i,2] - middle_points[i, 4]))
    bi <- c(bi, middle_points[i,2] - (m*middle_points[i,1]))
  }
  tangents_df <- data.frame(m_eq = round(mi, digits=4), b_eq = round(bi, digits=4)) %>% mutate(eq_tg = paste(m_eq, "*x", "+", b_eq, sep=""))

  b <- ggplot(data=clustered_points, mapping=aes(x = x_point, y = y_point))
  for(i in 1:length(mi)){
    b <- b + geom_function(fun = function(x, i) mi[i]*x + bi[i], args = list(i = i))
  }
  b + geom_text(mapping = aes(label=which_input, color = as.factor(n_cluster)), position = position_dodge(width=1),  size=2.5) + geom_point(data=clustered_points, mapping=aes(x = x_centroid, y = y_centroid , color = "centroid cluster")) + theme_light() + labs(color='Cluster')
}
```

My second approach, after failing in the attempt of drawing the exact limits of the polygons by the intersection of the lines that defines them, was to assign all possible points in the defined space I have (defined by the matrix I randomly created and introduced in the function) to the k clusters I asked the function to create. The concept behind this is that if I decided to divide my initial points to k clusters, once this clusters are defined, any point in the 2 dimension space could be assigned to those clusters by the same rule of most proximate centroid. So first I define the clusters with the matrix of points I have as input, and then divide the 2D space consequently with those clusters. This follows the "formal definition" as defined by https://en.wikipedia.org/wiki/Voronoi_diagram#Formal_definition, even though the mathematical way of obtaining those regions stated in that same Wikipedia page (Spanish version) is using the mediatrices, as I stated before. 
```{r}
plot_the_plane_Vdiagram <- function(matrix_df_mod, centroids_df, clustered_points_df){
    x_of_the_plane <- seq(min(matrix_df_mod$X1), max(matrix_df_mod$X1), length.out=200)
    y_of_the_plane <- seq(min(matrix_df_mod$X2), max(matrix_df_mod$X2), length.out=200)
    plane_points <- expand.grid(x_of_the_plane, y_of_the_plane)
    
    function_min_dist_plane <- function(point, centroids_df){
      distances <- c()
      which_i <- c()
      for(i in 1:nrow(centroids_df)){
        euclidean_dist <- sqrt(sum((plane_points[point, 1:2] - centroids_df[i, 1:2])^2))
        distances <- c(distances, euclidean_dist)
        which_i <- c(which_i, paste(i))
      }
      which_cluster_min <- which_i[which(distances==min(distances))]
      which_cluster_min
    }
      
    assigned_clusters_for_plane <- as.numeric(unlist(lapply(1:nrow(plane_points), function_min_dist_plane, centroids_df = centroids_df)))
    assigned_clusters_for_plane_df <- plane_points %>% mutate(n_cluster = assigned_clusters_for_plane) %>% dplyr::rename(x = Var1, y = Var2)
    ggplot(data=assigned_clusters_for_plane_df, mapping=aes(x = x, y = y, color = as.factor(n_cluster))) + geom_point(alpha=0.05) + geom_text(data=clustered_points_df, mapping=aes(x = x_point, y = y_point, label=which_input, color = as.factor(n_cluster)), position = position_dodge(width=1),  size=2.5) + geom_point(data=clustered_points_df, mapping = aes(x = x_centroid, y = y_centroid, color = "centroid cluster")) + theme_light() + labs(color='Cluster')
}
```

## Exercise 3 easy version: FASTQ analysis  
#### 10 points  
  
You can find 1000 sequences in fastq format in this link https://d28rh4a8wq0iu5.cloudfront.net/ads1/data/ERR037900_1.first1000.fastq.  
If you have any doubts about this exercise, you should have a look at FASTQC - this is a program commonly used to assess the quality of next generation sequencing experiments, similar to what you will be doing in this exercise. You can find the explanations of the graphs it produces here:  https://www.bioinformatics.babraham.ac.uk/projects/fastqc/Help/3%20Analysis%20Modules/  
  
#### Part 1:  
  
(1) Explain how the fastq file is organised and what information does it hold. (Yes, google. But please explain in your own words. What are the different lines? How many lines per sequence?)  

A FASTQ file is a text file that contains the sequence data from the clusters that pass filter on a flow cell in SBS (sequence by synthesis) platforms.
The sequencing can be done in two ways: single-read and pair-end sequencing. Single-read sequencing carries the creation of one Read1 (R1) FASTQ file for each sample in each flow cell lane. 
There are 2 types of sequencing: single-read and pair-end sequencing. For the first type (single-read), one Read 1 (R1) FASTQ file is created for each sample per flow cell lane. For a paired-end run, one R1 and one Read 2 (R2) FASTQ file are created for each sample for each lane. In the pair-end sequencing case, two FASTQ files are created (R1 and R2) per sample, with two single sequences written on them.
 
Each entry in a FASTQ files consists of 4 lines, and correspond to one sequence:
- A sequence identifier with information about the sequencing run and the cluster. Each sequence identifier line starts with @. The parts in which this line is divided can be extracted from any of the first lines of any of the sequences in the given FASTQ file:
  (whole line): @ERR037900.2 509.5.68.21343.17610/1
  ERR037900: unique instrument name
  2: run id
  509: flow cell id
  5: flow cell lane
  68: tile number within the flow
  21343: 'x'-coordinate of the cluster within the tile
  17610: 'y'-coordinate of the cluster within the tile
  1: member of a pair 
-The sequence (bases; A, C, T, G and N).
-A separator between sequences, which is a plus (+) sign.
-The base call (the bases A,C,T,G,N...) quality scores. These are Phred +33 encoded, using ASCII characters to represent the numerical quality scores.

This FASTQ files are useful as sequence input for alignment experiments and other secondary analysis software.

<span style="color:green"> 1/1 </span>
  
#### Part 2:  
  
(1) What is Phred quality score? Write the function that will extract quality scores for reads in numeric values (Sanger Phred+33). Plot the mean PER BASE (for every position) quality for reads. (Yes, google. But write your own code that will extract quality scores and plot the mean.) 

"A Phred quality score is a measure of the quality of the identification of the nucleobases generated by automated DNA sequencing", this is, a measure of the probability of a base of being identified incorrectly. So the higher the score, the higher the probability the nucleobases are being called correctly. It is encoded as a single ASCII character in order to reduce the weight of the generated file.

As it is asked to extract the quality scores, I also searched how this is done: replace the ASCII character to the actual ASCII value of the character and then subtract (-) 33.

```{r}
FastQ_file <- fread("https://d28rh4a8wq0iu5.cloudfront.net/ads1/data/ERR037900_1.first1000.fastq",header=F)

mean_per_base_quality <- function(file){
  Qscores_ASCII <- file %>% mutate(n_row = seq(1, nrow(.), by = 1)) %>% filter(n_row%%4==0) #Because, as I said, Q scores are in the 4th line for each sequence. 
  Qscores <- lapply(Qscores_ASCII$V1, function(x){ as.numeric(charToRaw(x))-33 })
  Qscores_dt <- as.data.table(Qscores)
  Mean_per_base_q_dt <- data.table(base_position = seq(1, length(Qscores[[1]]), by = 1), meanQ = (rowMeans(Qscores_dt)))
  
  ggplot(Mean_per_base_q_dt, aes(x = base_position, y = meanQ)) + geom_line() + xlab("Position (bp)") + ylab("Score") + theme_light()

}

mean_per_base_quality(FastQ_file)
```
<span style="color:green"> 1/1 </span>

#### Part 3: 
  
(2) Plot the percentage of A,C,T,G nucleotides for every position (==PER BASE) on a single graph, using lines of different colors. (see example_graph1 - it doesn't necessarily need to be colored or formatted this way, this is just an example)  

```{r}
extract_seq <- function(file){
  second_and_forth <- file %>% mutate(n_row = seq(1, nrow(.), by = 1)) %>% filter(n_row%%2==0)
  second <- second_and_forth %>% mutate(n_row = seq(1, nrow(.), by = 1)) %>% filter(n_row%%2!=0) #what was the 2nd line now is the 1st.
  second$V1
}

seq_StringSet <- DNAStringSet(extract_seq(FastQ_file)) 
seq_dt <- data.table(extract_seq(FastQ_file)) %>% rename(seq = V1)

separate_bases <- as.data.table(strsplit(seq_dt$seq, "", fixed = TRUE))  
separate_bases[, pos_base:=.I] #This would be the position in the sequence
separate_bases <- melt(separate_bases, id_vars = pos_base, measure.vars = colnames(separate_bases)[-1001])

seqs_by_position <- unlist(lapply(1:100, function(x){ paste(separate_bases[pos_base==x, value],collapse="")}))
seqs_by_position_StringSet <- DNAStringSet(seqs_by_position)
Freq_base_pos <- alphabetFrequency(seqs_by_position_StringSet, baseOnly=TRUE)
Freqs_dt <- as.data.table(Freq_base_pos[,-5]) #we only want the percentaje of A,C,G or T, (we remove the frequencies of "others")
Freqs_dt[,pos_base:=.I][, ":="(A_perc=A/rowSums(Freqs_dt)*100, C_perc=C/rowSums(Freqs_dt)*100, G_perc=G/rowSums(Freqs_dt)*100, T_perc=T/rowSums(Freqs_dt)*100)]

dt_to_plot <- melt(Freqs_dt, id.vars= "pos_base", measure.vars=c("A_perc","C_perc","G_perc","T_perc")) 

ggplot(dt_to_plot, aes(x = pos_base, y = value)) + geom_line(aes(colour=factor(variable))) + labs(colour="% nucleotides") + ylab("base % per position") + xlab("Position of base") + theme_light() 
```  
<font color="green">2/2 </font> 

#### Part 4:  
  
(1) For each base, plot a boxplot of quality across all reads. Do this for all bases. (see example_graph2 - it doesn’t need to be colored or formatted this way, this is just an example)  
```{r}
extract_quality_boxplot <- function(file){
  Qscores_ASCII <- file %>% mutate(n_row = seq(1, nrow(.), by = 1)) %>% filter(n_row%%4==0)
  Qscores <- lapply(Qscores_ASCII$V1, function(x){ as.numeric(charToRaw(x))-33 })
  Qscores_dt <- as.data.table(Qscores)
  Qscores_dt[,base_pos:=as.character(.I)]
  seq_dt <- data.table(extract_seq(file)) %>% rename(seq = V1)
  names <- c(paste0("V",seq(1,nrow(seq_dt), by=1)))
  melted_dt <- melt(Qscores_dt, id_vars = base_pos, measure.vars = names) %>% arrange(as.numeric(base_pos))
  melted_dt_means <- melted_dt %>% group_by(base_pos) %>% summarize(average = mean(value), median = median(value))  %>% arrange(as.numeric(base_pos), .by_group = FALSE)
  
  melted_dt$base_pos <- factor(melted_dt$base_pos, levels = seq(1, 100, by=1))
  ggplot(data = melted_dt, mapping = aes(x = base_pos, y = value)) + geom_boxplot(outlier.shape=NA) + geom_line(data = melted_dt_means, mapping = aes(x = base_pos, y = average, group=1), color="red") + geom_line(data = melted_dt_means, mapping = aes(x = base_pos, y = median, group=1), color="blue") + xlab("Position") + ylab("Qscores") + scale_x_discrete(breaks = c(1, 25, 50, 75, 99), labels = c("1", "25", "50", "75", "100")) + theme(legend.position ="none") + theme_light() 

}
extract_quality_boxplot(FastQ_file)

```
  <font color="green">1/1 </font> 
  
  
#### Part 5:  
  
(2) For each position, calculate and plot the estimated probability of a base being wrong. If you disregard the strange phenomenon observed in question 2, how does quality depend on the position in the read? Why does this happen? Explain having in mind the sequencing procedure - assume this was sequenced by Illumina technology.  
```{r}
prob_error <- function(Qscore){
  P <- 10^(-Qscore/10)
  P
}

prob_and_plot_error <- function(file){
  Qscores_ASCII <- file %>% mutate(n_row = seq(1, nrow(.), by = 1)) %>% filter(n_row%%4==0)
  Qscores <- lapply(Qscores_ASCII$V1, function(x){ as.numeric(charToRaw(x))-33 })
  Qscores_dt <- as.data.table(Qscores)
  Mean_per_base_q_dt <- data.table(base_pos = seq(1, length(Qscores[[1]]), by = 1), meanQ = (rowMeans(Qscores_dt)))
  dt_with_error <- Mean_per_base_q_dt %>% mutate(prob_error = prob_error(meanQ))
  
  ggplot(dt_with_error, aes(base_pos, prob_error)) + geom_line() + xlab("Position") + ylab("error probability") + theme_light()

}

prob_and_plot_error(FastQ_file)

```
  We see that the first bases have a probability of 0, or really close to 0, of having an error, until position around 65-75, where there is peak (point where the sequence is not annotated and there is a generic N base, but we ignore this point as asked), and after that, the probability of introducing an error has a positive slope. This means that the quality decreases as we increase the base position, so higher positions have higher probability of having an error.

  When analyzing closely the Illumina technology, we can see the reason behind this:
  Illumina uses a procedure called "sequencing by synthesis", which is based on washing away the chemicals after each cycle of the process. The nucleotides added to make copies of the original sequence fragments have a terminator cap, that acts as a blocker so only one nucleotide gets linked in each cycle of the sequencing process, only the complementary base at a time. After the detection of the fluorescence signal the blocker is removed and the next cycle can start, repeating the process. So the sequencing of each cluster is made by the detection of specific fluorescence signals.
  Nevertheless, this technique can fail in advanced cycles, either because the blocker of a nucleotide is not correctly removed after signal detection, so in the next cycle no nucleotide can bind to the sequence, and the same old fluorescence signal is detected once again in the new cycle, and consequently this read will be out of phase or a cycle behind the rest from that time on; or because a defect terminator cap, and in this case two nucleotides can bind in one cycle.

  These errors occur with a low probability, but with increasing read length they add up and pollute the light signal more and more. Since the light signal is used to calculate quality scores the asynchronous signal results in a decreasing sequence quality score.

<font color="green"> 2/2 </font>


#### Part 6:  
  
(1) Plot per sequence quality, like in the example graph.
```{r}
extract_seq_and_plot <- function(file){
  Qscores_ASCII <- file %>% mutate(n_row = seq(1, nrow(.), by = 1)) %>% filter(n_row%%4==0)
  Qscores <- lapply(Qscores_ASCII$V1, function(x){ as.numeric(charToRaw(x))-33 })
  Qscores_dt <- as.data.table(Qscores)
  mean_Qscore_read <- data.table(meanQ = colMeans(Qscores_dt))
  
  ggplot(mean_Qscore_read, aes(meanQ)) + geom_freqpoly(binwidth = 1,color="red",size=1) + xlab("Mean sequence quality (Phred score)") + theme_light() + ggtitle("Quality score distribution over all sequences") + ylab(NULL)
}

extract_seq_and_plot(FastQ_file)
```
<font color="green">1/1</font> 

(2) Plot per sequence GC content distribution along with theoretical distribution, like in the example graph.
```{r}
# Extract sequence (function already created)
seq <- extract_seq(FastQ_file)
seq_DNAStringSet <- DNAStringSet(seq)
#GC_freq <- data.table(alphabetFrequency(seq_DNAStringSet)) %>% select_("G", "C")
GC_freq <- data.table(letterFrequency(seq_DNAStringSet, c("G", "C")))
total_freq_GC <- data.table(GC=rowSums(GC_freq))

recount_GC_dt <- total_freq_GC %>% group_by(GC) %>% summarise(n = n()) 

mean_and_sd <- data.table(mean_GC_freq = mean(total_freq_GC$GC), sd_GC_freq = sd(total_freq_GC$GC, na.rm = TRUE))

theoretical_normal_distribution <- data.table(theoretical_GC_contents = round(rnorm(nrow(total_freq_GC), mean_and_sd$mean_GC_freq, mean_and_sd$sd_GC_freq))) %>% group_by(theoretical_GC_contents) %>% summarise(n = n())

ggplot(recount_GC_dt, mapping = aes(x = GC, y = n)) + geom_line(aes(color="red")) + theme_light() + geom_line(theoretical_normal_distribution, mapping = aes(x = theoretical_GC_contents, y = n, color = "blue")) + scale_color_discrete(name = "curve type", labels = c("Theoretical distribution", "GC counts per read"))

```
<font color="green">2/2 </font>  

  
## Exercise 3 difficult version: Motif finding   
#### 17.5 points  
  
In this assignment your task is to find the main DNA sequence motif that is bound by a transcription factor. Download the CTCF binding locations from the K562 cell line from the following link: http://hgdownload.cse.ucsc.edu/goldenPath/hg19/encodeDCC/wgEncodeUwTfbs/wgEncodeUwTfbsK562CtcfStdPkRep1.narrowPeak.gz (you have to unzip it).  
The data is mapped to the human hg19 reference. Order the data by the p value and select the top 500 peaks. Using the selected peaks, extract the sequence corresponding to the +/-250 bases around the center of the peak.  
Construct a Gibbs sampler algorithm for finding sequence motifs. Use the algorithm to find the 3 strongest motifs in the top 150 sequences data. What is the percentage of selected sequences that contain each motif? Plot the occurrence frequency of the motif for each position on the sequences. Plot the sequence LOGO of each motif.  
Using the JASPAR database (you can search the results on this page http://jaspar.genereg.net/ or you can use packages "JASPAR" and "TFBSTools" in R), try to annotate your motif (find the most similar motif).

